# Disclaimer: This function was generated by AI. Please review before using.
# Agent Name: model_training_agent
# Time Created: 2025-03-28 10:07:25

def model_trainer(data_df, run_id=None):
    import pandas as pd
    import numpy as np
    import os
    import pickle
    from sklearn.model_selection import train_test_split, cross_validate
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer
    import warnings
    import mlflow
    from mlflow.tracking import MlflowClient
    # All imports at the beginning of the function, one per line











    # Silence warnings if needed
    warnings.filterwarnings('ignore')

    # Track execution success
    execution_successful = False

    try:
        # Extract features and target
        X = data_df.drop('target', axis=1)
        y = data_df['target']

        # Split the data into training and test sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

        # Data Preprocessing: Scaling
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Model Training: Logistic Regression with cross-validation
        model = LogisticRegression(random_state=42, solver='liblinear')

        # Define the scoring metrics
        scoring = {
            'accuracy': make_scorer(accuracy_score),
            'precision': make_scorer(precision_score),
            'recall': make_scorer(recall_score),
            'f1': make_scorer(f1_score)
        }

        # Perform cross-validation
        cv_results = cross_validate(model, X_train_scaled, y_train, cv=5, scoring=scoring)

        # Train the model on the entire training set after cross-validation
        model.fit(X_train_scaled, y_train)

        # Make predictions on the test set
        y_pred = model.predict(X_test_scaled)

        # Calculate metrics on the test set
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # If we get here without exceptions, execution was successful
        execution_successful = True

        # Prepare return dictionary
        results = {
            'trained_model': model,
            'metrics': {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'cv_accuracy_mean': cv_results['test_accuracy'].mean(),
                'cv_precision_mean': cv_results['test_precision'].mean(),
                'cv_recall_mean': cv_results['test_recall'].mean(),
                'cv_f1_mean': cv_results['test_f1'].mean(),
                'cv_accuracy_std': cv_results['test_accuracy'].std(),
                'cv_precision_std': cv_results['test_precision'].std(),
                'cv_recall_std': cv_results['test_recall'].std(),
                'cv_f1_std': cv_results['test_f1'].std(),
            },
            'predictions': y_pred,
            'X_test': X_test,
            'y_test': y_test,
            'scaler': scaler
        }

    except Exception as e:
        print(f"Error during model training: {e}")
        return {
            'trained_model': None,
            'metrics': {},
            'predictions': None,
            'error': str(e)
        }

    finally:
        # Only log to MLflow if execution was successful AND run_id is provided
        if execution_successful and run_id is not None:
            # Create MLflow client
            client = MlflowClient()

            # Log model parameters with function_name prefix
            for param_name, param_value in model.get_params().items():
                client.log_param(run_id, f"model_trainer_model_{param_name}", param_value)

            # Log all metrics from our results with function_name prefix
            for metric_name, metric_value in results['metrics'].items():
                client.log_metric(run_id, f"model_trainer_{metric_name}", metric_value)

            # Save model to disk temporarily and log as artifact
            if results['trained_model'] is not None:
                model_path = "trained_model.pkl"
                try:
                    with open(model_path, "wb") as f:
                        pickle.dump(results['trained_model'], f)
                    client.log_artifact(run_id, model_path)
                    # Clean up the temporary file
                    if os.path.exists(model_path):
                        os.remove(model_path)
                except Exception as e:
                    print(f"Error saving model: {e}")

            # Save scaler to disk temporarily and log as artifact
            if results.get('scaler') is not None:
                scaler_path = "scaler.pkl"
                try:
                    with open(scaler_path, "wb") as f:
                        pickle.dump(results['scaler'], f)
                    client.log_artifact(run_id, scaler_path)
                    # Clean up the temporary file
                    if os.path.exists(scaler_path):
                        os.remove(scaler_path)
                except Exception as e:
                    print(f"Error saving scaler: {e}")

        # Return results
        return results