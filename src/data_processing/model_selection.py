# Disclaimer: This function was generated by AI. Please review before using.
# Agent Name: model_selection_agent
# Time Created: 2025-03-28 10:07:09

def model_selection(data_features, data_target, run_id=None):
    import pandas as pd
    import numpy as np
    import os
    import pickle
    from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, roc_curve  # for classification
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # for classification
    import xgboost as xgb
    from sklearn.svm import SVC
    from sklearn.neural_network import MLPClassifier
    import mlflow
    from mlflow.tracking import MlflowClient
    import warnings
    '''
    Model selection function that compares multiple algorithms and returns the best model.
    
    Parameters:
    ----------
    data_features : pandas.DataFrame
        DataFrame containing the feature columns
    data_target : pandas.Series or pandas.DataFrame
        Target variable for prediction
    run_id : str, optional
        MLflow run ID to use for logging metrics and parameters
        
    Returns:
    -------
    dict
        Dictionary with best model, metrics, and related information
    '''














    # Silence warnings
    warnings.filterwarnings('ignore')
    
    # Track execution success
    execution_successful = False
    
    try:
        # Define models
        models = {
            'LogisticRegression': LogisticRegression(random_state=42, solver='liblinear'),
            'RandomForest': RandomForestClassifier(random_state=42),
            'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
            'SVM': SVC(random_state=42, probability=True),
            'NeuralNetwork': MLPClassifier(random_state=42, max_iter=300)
        }
        
        # Stratified K-Fold Cross-Validation
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        # Evaluate each model
        results = {}
        for model_name, model in models.items():
            
            # Cross-validation
            recall_scores = []
            precision_scores = []
            f1_scores = []
            accuracy_scores = []
            auc_roc_scores = []
            
            for fold, (train_index, val_index) in enumerate(cv.split(data_features, data_target)):
                X_train, X_val = data_features.iloc[train_index], data_features.iloc[val_index]
                y_train, y_val = data_target.iloc[train_index], data_target.iloc[val_index]
                
                model.fit(X_train, y_train)
                y_pred = model.predict(X_val)
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                
                recall = recall_score(y_val, y_pred)
                precision = precision_score(y_val, y_pred)
                f1 = f1_score(y_val, y_pred)
                accuracy = accuracy_score(y_val, y_pred)
                auc_roc = roc_auc_score(y_val, y_pred_proba)
                
                recall_scores.append(recall)
                precision_scores.append(precision)
                f1_scores.append(f1)
                accuracy_scores.append(accuracy)
                auc_roc_scores.append(auc_roc)
                
            results[model_name] = {
                'recall_mean': np.mean(recall_scores),
                'recall_std': np.std(recall_scores),
                'precision_mean': np.mean(precision_scores),
                'precision_std': np.std(precision_scores),
                'f1_mean': np.mean(f1_scores),
                'f1_std': np.std(f1_scores),
                'accuracy_mean': np.mean(accuracy_scores),
                'accuracy_std': np.std(accuracy_scores),
                'auc_roc_mean': np.mean(auc_roc_scores),
                'auc_roc_std': np.std(auc_roc_scores),
                'model': model
            }
        
        # Select best model based on recall
        best_model_name = max(results, key=lambda x: results[x]['recall_mean'])
        best_model = results[best_model_name]['model']
        
        # Retrain best model on all data
        best_model.fit(data_features, data_target)
        
        # Evaluate on the entire dataset
        y_pred = best_model.predict(data_features)
        y_pred_proba = best_model.predict_proba(data_features)[:, 1]
        
        final_recall = recall_score(data_target, y_pred)
        final_precision = precision_score(data_target, y_pred)
        final_f1 = f1_score(data_target, y_pred)
        final_accuracy = accuracy_score(data_target, y_pred)
        final_auc_roc = roc_auc_score(data_target, y_pred_proba)
        
        # Create results dictionary
        final_results = {
            'best_model': best_model,
            'best_model_name': best_model_name,
            'metrics': {
                'recall': final_recall,
                'precision': final_precision,
                'f1': final_f1,
                'accuracy': final_accuracy,
                'auc_roc': final_auc_roc
            },
            'all_models': results
        }
        
        # If we reach this point without errors, execution was successful
        execution_successful = True
        
        return final_results
        
    except Exception as e:
        print(f"Error during model selection: {e}")
        return {
            'best_model': None,
            'best_model_name': None,
            'metrics': {},
            'error': str(e)
        }
        
    finally:
        # Only log to MLflow if execution was successful AND run_id is provided
        if execution_successful and run_id is not None:
            # Create MLflow client 
            client = MlflowClient()
            
            # Log all models evaluated with function_name prefix
            client.log_param(run_id, f"model_selection_models_evaluated", str(list(results.keys())))
            client.log_param(run_id, f"model_selection_best_model_type", best_model_name)
            
            # Log metrics for all models with function_name prefix
            for model_name, model_results in results.items():
                client.log_metric(run_id, f"model_selection_model_recall_mean", model_results['recall_mean'])
                client.log_metric(run_id, f"model_selection_model_recall_std", model_results['recall_std'])
                client.log_metric(run_id, f"model_selection_model_precision_mean", model_results['precision_mean'])
                client.log_metric(run_id, f"model_selection_model_precision_std", model_results['precision_std'])
                client.log_metric(run_id, f"model_selection_model_f1_mean", model_results['f1_mean'])
                client.log_metric(run_id, f"model_selection_model_f1_std", model_results['f1_std'])
                client.log_metric(run_id, f"model_selection_model_accuracy_mean", model_results['accuracy_mean'])
                client.log_metric(run_id, f"model_selection_model_accuracy_std", model_results['accuracy_std'])
                client.log_metric(run_id, f"model_selection_model_auc_roc_mean", model_results['auc_roc_mean'])
                client.log_metric(run_id, f"model_selection_model_auc_roc_std", model_results['auc_roc_std'])
            
            # Log best model parameters with function_name prefix
            for param_name, param_value in best_model.get_params().items():
                if isinstance(param_value, (str, int, float, bool)):
                    client.log_param(run_id, f"model_selection_best_model_param_{param_name}", param_value)
            
            # Save model to disk temporarily and log as artifact
            if best_model is not None:
                model_path = "best_model.pkl"
                try:
                    with open(model_path, "wb") as f:
                        pickle.dump(best_model, f)
                    client.log_artifact(run_id, model_path)
                    # Clean up the temporary file
                    if os.path.exists(model_path):
                        os.remove(model_path)
                except Exception as e:
                    print(f"Error saving model: {e}")